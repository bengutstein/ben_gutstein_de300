{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f368aed-b783-4a85-bb50-b77d867f7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Critical Disclosure:\n",
    "I was able to run all of this locally, but it did not work in the docker. \n",
    "Within this folder, there is a docker and a local version of the ipynb. \n",
    "The local version should have the results I attained while running the functions\n",
    "on my machine. Thank you for understanding. Please reach out if there is a problem\n",
    "because I had some trouble with my EC2 instance while working on this HW\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f0180-3525-4e10-a340-5d96f9313820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7e8b5-1ecb-4aed-a86d-9e288d6284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5183274-f395-4b31-bbcb-6e1dffb2fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $JAVA_HOME\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ff5c0-8f20-4e81-a931-5cb29957bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# turning the second column from a string to an array\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172eb0b3-86f2-4c1a-a8cf-6d66515c56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19d30e-7325-44c2-8a62-4566013197e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD with (doc_id, filtered_words)\n",
    "print(\"Creating RDD...\")\n",
    "rdd = agnews.select(\"_c0\", \"filtered\").rdd\n",
    "\n",
    "#Word-document frequency (for TF)\n",
    "print(\"Mapping word-document pairs...\")\n",
    "word_doc_pairs = rdd.flatMap(lambda row: [((word, row[\"_c0\"]), 1) for word in row[\"filtered\"]])\n",
    "\n",
    "print(\"Reducing term counts...\")\n",
    "term_counts = word_doc_pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Document lengths (used to normalize TF)\n",
    "print(\"Calculating document lengths...\")\n",
    "doc_lengths = rdd.map(lambda row: (row[\"_c0\"], len(row[\"filtered\"])))\n",
    "\n",
    "# Monitor partitions to ensure Spark is progressing (Check I added when things were moving slowly, not necessary on my local machine\n",
    "doc_lengths.foreachPartition(lambda part: print(\"Processed one doc_length partition.\"))\n",
    "\n",
    "doc_lengths_dict = dict(doc_lengths.collect())\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "print(\"Computing TF...\")\n",
    "tf = term_counts.map(lambda x: (x[0], x[1] / doc_lengths_dict[x[0][1]]))\n",
    "\n",
    "# Compute Document Frequencies for each term\n",
    "print(\"Calculating document frequency for each term...\")\n",
    "unique_word_doc = rdd.flatMap(lambda row: [(word, row[\"_c0\"]) for word in set(row[\"filtered\"])])\n",
    "doc_freq = unique_word_doc.distinct().map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Log document count (another status check)\n",
    "N = agnews.count()\n",
    "print(f\"Total documents: {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d886da1-aa6f-4398-befb-6d3aa98a6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# IDF Collection using doc_freq which is a PipelinedRDD\n",
    "print(\"Collecting document frequencies and computing IDF in-memory...\")\n",
    "idf_dict = dict(doc_freq.collect())\n",
    "idf_dict = {term: math.log(N / df) for term, df in idf_dict.items()}\n",
    "\n",
    "# Join TF and IDF\n",
    "print(\"Computing TF-IDF...\")\n",
    "tfidf = tf.map(lambda x: (x[0], x[1] * idf_dict[x[0][0]]))\n",
    "\n",
    "# Group TF-IDF scores by document\n",
    "print(\"Grouping TF-IDF scores by document ID...\")\n",
    "doc_tfidf = tfidf.map(lambda x: (x[0][1], [(x[0][0], x[1])])) \\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first 5 TF-IDF results\n",
    "print(\"\\nTF-IDF scores for first 5 documents:\")\n",
    "for doc_id, scores in doc_tfidf.take(5):\n",
    "    print(f\"Doc {doc_id}: {sorted(scores, key=lambda x: -x[1])[:5]}\")  # top 5 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a31b9-ac74-49b2-8c8d-97beff80fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2:SVM\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f1ee-8117-40cc-b9e7-a9613fa7c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example to read the files. But you should consider using pyspark directly. \n",
    "# *Make sure you are not assuming a header*!!\n",
    "import pandas as pd\n",
    "data_svm = pd.read_csv('data_for_svm.csv', header=None)\n",
    "w = pd.read_csv('w.csv', header=None)\n",
    "bias = pd.read_csv('bias.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2063ef1-f43b-4b11-904a-c2b4774bac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_SVM(w, b, X, y, lambd=1.0):\n",
    "    \"\"\"\n",
    "    Compute the SVM loss:\n",
    "    L(w, b) = lambda * ||w||^2 + (1/n) * sum_i max(0, 1 - y_i * (w^T x_i + b))\n",
    "    \n",
    "    Parameters:\n",
    "    - w: weight vector (64, 1)\n",
    "    - b: bias (scalar)\n",
    "    - X: feature matrix (n, 64)\n",
    "    - y: labels (n,)\n",
    "    - lambd: regularization parameter\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    w = w.values.flatten()\n",
    "    b = bias.values.flatten()[0]\n",
    "    X = X.iloc[:, :-1].values\n",
    "    y = X = data_svm.iloc[:, :-1].values\n",
    "    y = data_svm.iloc[:, -1].values\n",
    "    \n",
    "    # Compute hinge loss terms: max(0, 1 - y_i * (w^T x_i + b))\n",
    "    margins = 1 - y * (X @ w + b)\n",
    "    hinge_loss = np.maximum(0, margins)\n",
    "\n",
    "    # Final loss\n",
    "    loss = lambd * np.linalg.norm(w)**2 + hinge_loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7954f4-95f6-441c-acb3-9cc3ed47c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_SVM(w, bias, data_svm, data_svm.iloc[:, -1])\n",
    "print(\"SVM Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf00fa7-b474-4e27-815b-151ff94dda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_SVM(w, b, X):\n",
    "    \"\"\"\n",
    "    Predicts labels using the SVM decision rule: sign(w^T x + b)\n",
    "    \n",
    "    Parameters:\n",
    "    - w: weight vector (64,)\n",
    "    - b: scalar bias\n",
    "    - X: feature matrix (n, 64)\n",
    "    \n",
    "    Returns:\n",
    "    - y_hat: predicted labels (n,), each ∈ {−1, +1}\n",
    "    \"\"\"\n",
    "    w = w.values.flatten()\n",
    "    b = b.values.flatten()[0]\n",
    "    X = X.iloc[:, :-1].values  # only features, drop label column\n",
    "\n",
    "    scores = X.dot(w) + b\n",
    "    y_hat = np.sign(scores)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826195f-0836-42aa-94df-754708509935",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_SVM(w, bias, data_svm)\n",
    "print(\"Predictions:\\n\", y_pred[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
