{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4f02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/bengutstein/anaconda3/lib/python3.10/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/bengutstein/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d30bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33.2M  100 33.2M    0     0  29.9M      0  0:00:01  0:00:01 --:--:-- 29.9M\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb83ba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "java version \"1.8.0_451\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_451-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.451-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bd14e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 13:58:06 WARN Utils: Your hostname, MacBook-Pro-1120.local resolves to a loopback address: 127.0.0.1; using 10.0.0.8 instead (on interface en0)\n",
      "25/05/22 13:58:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 13:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# turning the second column from a string to an array\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd0a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|_c0|                      filtered|\n",
      "+---+------------------------------+\n",
      "|  0|[wall, st, bears, claw, bac...|\n",
      "|  1|[carlyle, looks, toward, co...|\n",
      "|  2|[oil, economy, cloud, stock...|\n",
      "|  3|[iraq, halts, oil, exports,...|\n",
      "|  4|[oil, prices, soar, time, r...|\n",
      "+---+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 13:58:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bengutstein/Desktop/homework3/agnews_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RDD...\n",
      "Mapping word-document pairs...\n",
      "Reducing term counts...\n",
      "Calculating document lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 13:58:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bengutstein/Desktop/homework3/agnews_clean.csv\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.Processed one doc_length partition.\n",
      "\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.\n",
      "Processed one doc_length partition.\n",
      "25/05/22 13:58:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bengutstein/Desktop/homework3/agnews_clean.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TF...\n",
      "Calculating document frequency for each term...\n",
      "Total documents: 127600\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Create an RDD with (doc_id, filtered_words)\n",
    "print(\"Creating RDD...\")\n",
    "rdd = agnews.select(\"_c0\", \"filtered\").rdd\n",
    "\n",
    "# STEP 5: Word-document frequency (for TF)\n",
    "print(\"Mapping word-document pairs...\")\n",
    "word_doc_pairs = rdd.flatMap(lambda row: [((word, row[\"_c0\"]), 1) for word in row[\"filtered\"]])\n",
    "\n",
    "print(\"Reducing term counts...\")\n",
    "term_counts = word_doc_pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Document lengths (for normalizing TF)\n",
    "print(\"Calculating document lengths...\")\n",
    "doc_lengths = rdd.map(lambda row: (row[\"_c0\"], len(row[\"filtered\"])))\n",
    "\n",
    "# Monitor partitions to ensure Spark is progressing\n",
    "doc_lengths.foreachPartition(lambda part: print(\"Processed one doc_length partition.\"))\n",
    "\n",
    "doc_lengths_dict = dict(doc_lengths.collect())\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "print(\"Computing TF...\")\n",
    "tf = term_counts.map(lambda x: (x[0], x[1] / doc_lengths_dict[x[0][1]]))\n",
    "\n",
    "# Compute Document Frequencies for each term\n",
    "print(\"Calculating document frequency for each term...\")\n",
    "unique_word_doc = rdd.flatMap(lambda row: [(word, row[\"_c0\"]) for word in set(row[\"filtered\"])])\n",
    "doc_freq = unique_word_doc.distinct().map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Log document count\n",
    "N = agnews.count()\n",
    "print(f\"Total documents: {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting document frequencies and computing IDF in-memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 13:58:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bengutstein/Desktop/homework3/agnews_clean.csv\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "[Stage 8:======>                                                    (1 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23346Kb max_used=23354Kb free=107725Kb\n",
      " bounds [0x00000001071f8000, 0x00000001088e8000, 0x000000010f1f8000]\n",
      " total_blobs=9360 nmethods=8405 adapters=867\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 13:58:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , filtered\n",
      " Schema: _c0, filtered\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/bengutstein/Desktop/homework3/agnews_clean.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TF-IDF...\n",
      "Grouping TF-IDF scores by document ID...\n",
      "\n",
      "TF-IDF scores for first 5 documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0: [('cynics', 0.563734318747707), ('wall', 0.5115985326511431), ('claw', 0.499114829314058), ('dwindling', 0.4572386180709258), ('sellers', 0.4468379768438066)]\n",
      "Doc 9: [('cynics', 0.5340640914451961), ('wall', 0.48467229409055657), ('claw', 0.47284562777121286), ('dwindling', 0.43317342764614025), ('sellers', 0.4233201885888694)]\n",
      "Doc 18: [('deficit', 0.540640233790213), ('swells', 0.4456552560521169), ('trade', 0.4080689115854956), ('8bn', 0.39631754501941335), ('imports', 0.31527891489726495)]\n",
      "Doc 27: [('fall', 0.43167912777209483), ('shares', 0.386492653524941), ('tumble', 0.3419973653042592), ('quarter', 0.3411920291869863), ('disappointing', 0.29431935936888143)]\n",
      "Doc 36: [('google', 0.34612714944621187), ('secrecy', 0.3432093194552543), ('confusing', 0.3299462238286493), ('submitted', 0.28716139264401674), ('news', 0.27869755632552773)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# IDF Collection\n",
    "print(\"Collecting document frequencies and computing IDF in-memory...\")\n",
    "idf_dict = dict(doc_freq.collect())\n",
    "idf_dict = {term: math.log(N / df) for term, df in idf_dict.items()}\n",
    "\n",
    "# Join TF and IDF\n",
    "print(\"Computing TF-IDF...\")\n",
    "tfidf = tf.map(lambda x: (x[0], x[1] * idf_dict[x[0][0]]))\n",
    "\n",
    "# Group TF-IDF scores by document\n",
    "print(\"Grouping TF-IDF scores by document ID...\")\n",
    "doc_tfidf = tfidf.map(lambda x: (x[0][1], [(x[0][0], x[1])])) \\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first 5 TF-IDF results\n",
    "print(\"\\nTF-IDF scores for first 5 documents:\")\n",
    "for doc_id, scores in doc_tfidf.take(5):\n",
    "    print(f\"Doc {doc_id}: {sorted(scores, key=lambda x: -x[1])[:5]}\")  # top 5 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bae050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n"
     ]
    }
   ],
   "source": [
    "# Part 2:SVM\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45df7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2p/zx33gyp17_v32t3njhf3w0fr0000gn/T/ipykernel_71638/2314691258.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# This is an example to read the files. But you should consider using pyspark directly. \n",
    "# *Make sure you are not assuming a header*!!\n",
    "import pandas as pd\n",
    "data_svm = pd.read_csv('data_for_svm.csv', header=None)\n",
    "w = pd.read_csv('w.csv', header=None)\n",
    "bias = pd.read_csv('bias.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a6d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_SVM(w, b, X, y, lambd=1.0):\n",
    "    \"\"\"\n",
    "    Compute the SVM loss:\n",
    "    L(w, b) = lambda * ||w||^2 + (1/n) * sum_i max(0, 1 - y_i * (w^T x_i + b))\n",
    "    \n",
    "    Parameters:\n",
    "    - w: weight vector (64, 1)\n",
    "    - b: bias (scalar)\n",
    "    - X: feature matrix (n, 64)\n",
    "    - y: labels (n,)\n",
    "    - lambd: regularization parameter\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    w = w.values.flatten()\n",
    "    b = bias.values.flatten()[0]\n",
    "    X = X.iloc[:, :-1].values\n",
    "    y = X = data_svm.iloc[:, :-1].values\n",
    "    y = data_svm.iloc[:, -1].values\n",
    "    \n",
    "    # Compute hinge loss terms: max(0, 1 - y_i * (w^T x_i + b))\n",
    "    margins = 1 - y * (X @ w + b)\n",
    "    hinge_loss = np.maximum(0, margins)\n",
    "\n",
    "    # Final loss\n",
    "    loss = lambd * np.linalg.norm(w)**2 + hinge_loss.mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e0a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Loss: 1.0029403834857522\n"
     ]
    }
   ],
   "source": [
    "loss = loss_SVM(w, bias, data_svm, data_svm.iloc[:, -1])\n",
    "print(\"SVM Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404dd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_SVM(w, b, X):\n",
    "    \"\"\"\n",
    "    Predicts labels using the SVM decision rule: sign(w^T x + b)\n",
    "    \n",
    "    Parameters:\n",
    "    - w: weight vector (64,)\n",
    "    - b: scalar bias\n",
    "    - X: feature matrix (n, 64)\n",
    "    \n",
    "    Returns:\n",
    "    - y_hat: predicted labels (n,), each ∈ {−1, +1}\n",
    "    \"\"\"\n",
    "    w = w.values.flatten()\n",
    "    b = b.values.flatten()[0]\n",
    "    X = X.iloc[:, :-1].values  # only features, drop label column\n",
    "\n",
    "    scores = X.dot(w) + b\n",
    "    y_hat = np.sign(scores)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95be82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [-1. -1. -1.  1. -1.  1. -1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_SVM(w, bias, data_svm)\n",
    "print(\"Predictions:\\n\", y_pred[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
